{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook Data Pipelines et Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 classes:**\n",
    "- Data Handler\n",
    "- Feature Recipe\n",
    "- Future Extactor\n",
    "- Model Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "- https://storage.googleapis.com/h3-data/listings_final.csv\n",
    "- https://storage.googleapis.com/h3-data/price_availability.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer vos librairies  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Toutes les Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "        Getting data from bucket\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Initialising the 3 datasets :\n",
    "            entry 1\n",
    "            entry 2\n",
    "            result\n",
    "        \"\"\"\n",
    "        print(\"DataHandlerintialisation\")\n",
    "        self.df_lf = None\n",
    "        self.df_pa = None\n",
    "        self.df_res = None\n",
    "        print(\"intialisation done\")\n",
    "\n",
    "    def get_data(self):\n",
    "        print(\"loading data from bucket\")\n",
    "        self.df_lf = pd.read_csv(\"https://storage.googleapis.com/h3-data/listings_final.csv\",sep=';')\n",
    "        self.df_pa = pd.read_csv(\"https://storage.googleapis.com/h3-data/price_availability.csv\",sep=';')\n",
    "        print(\"data loaded from bucket\")\n",
    "\n",
    "    def group_data(self):\n",
    "        print(\"merging data\")\n",
    "        self.df_res = pd.merge(self.df_lf,self.df_pa.groupby('listing_id')['local_price'].mean('local_price'),how='inner', on='listing_id')\n",
    "        print(\"size of the merged data : {} lines, {} columns\".format(self.df_res.shape[0],self.df_res.shape[1]))\n",
    "\n",
    "    def get_process_data(self):\n",
    "        self.get_data()\n",
    "        self.group_data()\n",
    "        print(\"end of processing\\n\")\n",
    "\n",
    "class FeatureRecipe:\n",
    "\n",
    "    def __init__(self,df:pd.DataFrame):\n",
    "        print(\"FeatureRecipe intialisation\")\n",
    "        self.df = df\n",
    "        self.cate = []\n",
    "        self.floa = []\n",
    "        self.intt = []\n",
    "        self.drop = []\n",
    "        print(\"end of intialisation\")\n",
    "\n",
    "    def separate_variable_types(self) -> None:\n",
    "        print(\"separating columns\")\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtypes == int:\n",
    "                self.intt.append(self.df[col])\n",
    "            elif self.df[col].dtypes == float:\n",
    "                self.floa.append(self.df[col])\n",
    "            else:\n",
    "                self.cate.append(self.df[col])\n",
    "        print (\"dataset column size : {} \\nnumber of discreet values : {} \\nnumber of continuous values : {} \\nnumber of others : {} \\ntaille total : {}\".format(len(self.df.columns),len(self.intt),len(self.floa),len(self.cate),len(self.intt)+len(self.floa)+len(self.cate) ))\n",
    "\n",
    "    def drop_na_prct(self,threshold : float):\n",
    "        \"\"\"\n",
    "            on appelle la commande et on met un threshold entre 1 et 0 en flottant\n",
    "            params: threshold : float\n",
    "        \"\"\"\n",
    "        # par rapport a la colonne\n",
    "        dropped = 0\n",
    "        print(\"dropping columns with {} percentage \".format(threshold))\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isna().sum()/self.df.shape[0] >= threshold:\n",
    "                self.drop.append( self.df.drop([col], axis='columns', inplace=True) )\n",
    "                dropped+=1\n",
    "        print(\"dropped {} columns\".format(dropped))\n",
    "\n",
    "    def drop_useless_features(self):\n",
    "        # droper les col vides et doublons de l'index et les colonnes qu'on va considerer inutile\n",
    "        print(\"dropping useless columns\")\n",
    "        if 'Unnamed: 0' in self.df.columns:\n",
    "            self.df.drop(['Unnamed: 0'], axis='columns', inplace=True)\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isna().sum() == len(self.df):\n",
    "                self.df.drop([col], axis='columns', inplace=True)\n",
    "        print(\"done dropping\")\n",
    "\n",
    "    def drop_duplicate(self):\n",
    "        # comparer les colonnes et voir si elles sont dupliquées\n",
    "        print(\"dropping duplicated rows\")\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "        duplicates = self.get_duplicates()\n",
    "        for col in duplicates:\n",
    "            self.df.drop(col)\n",
    "        print(\"duplicated rows dropped\")\n",
    "\n",
    "    def get_duplicates(self):\n",
    "        duplicates = []\n",
    "        #for col in self.df.columns:\n",
    "            #for scol in self.df.columns:\n",
    "        for col in range(self.df.shape[1]-1):\n",
    "            for scol in range(col+1,self.df.shape[1]-1):\n",
    "                print(\"{} {}\".format(col,scol))\n",
    "                if sum( np.where(self.df.iloc[:,[col]] == self.df.iloc[:,[scol]],0,1) ) == 0:\n",
    "                    duplicates.append(scol)\n",
    "        return duplicates\n",
    "\n",
    "    # def deal_date_time(self):\n",
    "    #   pass\n",
    "\n",
    "    def get_process_data(self,threshold : float):\n",
    "        self.drop_useless_features()\n",
    "        self.drop_na_prct(threshold)\n",
    "        self.drop_duplicate()\n",
    "        self.separate_variable_types()\n",
    "        print(\"end of FeatureRecipe processing\\n\")\n",
    "\n",
    "import sklearn as skn\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature Extractor class\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, flist: list):\n",
    "        \"\"\"\n",
    "            Input : pandas.DataFrame, feature list to drop\n",
    "            Output : X_train, X_test, y_train, y_test according to sklearn.model_selection.train_test_split\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None,None,None,None\n",
    "        self.df = data\n",
    "        self.flist = flist\n",
    "    def splitting(size:float,rng:int,X pd.Series):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test train_test_split(X, y, size, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Classe DataHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 attributs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \"\"\"\n",
    "        Get data from GSC Bucket \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.csvfile1 = None\n",
    "        self.csvfile2 = None\n",
    "        self.gouped_data = None\n",
    "    def get_data(self):\n",
    "        print(\" - - - fetch data from gcs bucket : - - - \")\n",
    "        listing = pd.read_csv('https://storage.googleapis.com/h3-data/listings_final.csv', sep=';')\n",
    "        prices = pd.read_csv('https://storage.googleapis.com/h3-data/price_availability.csv', sep=';')\n",
    "        self.csvfile1, self.csvfile2 = listing, prices  \n",
    "        return \" - - - data loaded - - - \\nFiles : \\n  - listing {} \\n  - prices {}\".format(listing.shape,prices.shape)\n",
    "    def group_data(self):\n",
    "        data = self.csvfile2.groupby('listing_id')['local_price'].mean()\n",
    "        self.gouped_data = pd.merge(data, self.csvfile1, on='listing_id')\n",
    "        print(\" - - - data merged - - - \")\n",
    "    def get_process_data(self):\n",
    "        self.get_data()\n",
    "        self.group_data()\n",
    "        print(\" - - - data processed - - - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \"\"\"\n",
    "        Getting data from bucket\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Initialising the 3 datasets :\n",
    "            entry 1\n",
    "            entry 2\n",
    "            result\n",
    "        \"\"\"\n",
    "        print(\"DataHandlerintialisation\")\n",
    "        self.df_lf = None\n",
    "        self.df_pa = None\n",
    "        self.df_res = None\n",
    "        print(\"intialisation done\")\n",
    "\n",
    "    def get_data(self):\n",
    "        print(\"loading data from bucket\")\n",
    "        self.df_lf = pd.read_csv(\"https://storage.googleapis.com/h3-data/listings_final.csv\",sep=';')\n",
    "        self.df_pa = pd.read_csv(\"https://storage.googleapis.com/h3-data/price_availability.csv\",sep=';')\n",
    "        print(\"data loaded from bucket\")\n",
    "\n",
    "    def group_data(self):\n",
    "        print(\"merging data\")\n",
    "        self.df_res = pd.merge(self.df_lf,self.df_pa.groupby('listing_id')['local_price'].mean('local_price'),how='inner', on='listing_id')\n",
    "        print(\"size of the merged data : {} lines, {} columns\".format(self.df_res.shape[0],self.df_res.shape[1]))\n",
    "\n",
    "    def get_process_data(self):\n",
    "        self.get_data()\n",
    "        self.group_data()\n",
    "        print(\"end of processing\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7fdefcfe09f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'DataHandler' is not defined"
     ]
    }
   ],
   "source": [
    "d1 = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-807b84d32f02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "d1.get_process_data()\n",
    "%time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.df_merge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.df_merge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Classe FutureRecipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-322c2d7aa4a5>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-322c2d7aa4a5>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    \"\"\" TODO : Diviser les types de variables dans un tableau \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class FeatureRecipe:\n",
    "    \"\"\"\n",
    "    Feature processing class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        self.continuous = None\n",
    "        self.categorical = None\n",
    "        self.discrete = None\n",
    "        self.datetime = None\n",
    "        \n",
    "    #Separate features for feature engineering\n",
    "    def separate_vraible_types(self) -> None:\n",
    "    \"\"\" TODO : Diviser les types de variables dans un tableau \"\"\"\n",
    "        pass   \n",
    "    \n",
    "    #Useless feature\n",
    "    def drop_uselessf(self):\n",
    "    \"\"\" TODO : Supprimer les colonnes inutiles du dataset \"\"\"\n",
    "    #Soit celles en double soit celles avec NaN de 100%\n",
    "        pass\n",
    "    \n",
    "    def deal_duplicate(self):\n",
    "    \"\"\" TODO : Supprimer les lignes dupliquées du dataset \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #NaN\n",
    "    def drop_nanp(self, thresold: float):\n",
    "    \"\"\" TODO : Supprimer un pourcentage de NA du dataset \"\"\"\n",
    "        \"\"\"\n",
    "        Drop NaN columns according to a thresold\n",
    "        \"\"\"\n",
    "        def deal_nanp(df:pd.DataFrame, thresold: float):\n",
    "            bf=[]\n",
    "            for c in self.data.columns.to_list():\n",
    "                if self.data[c].isna().sum()/self.data.shape[0] > thresold:\n",
    "                    bf.append(c)\n",
    "            logging.debug(\"{} feature have more than {} NaN \".format(len(bf), thresold))\n",
    "            logging.debug('\\n\\n - - - features - - -  \\n {}'.format(bf))\n",
    "            return bf \n",
    "        self.data = self.data.drop(deal_nanp(self.data, thresold), axis=1)\n",
    "        logging.debug('Some NaN features droped according to {} thresold'.format(thresold))\n",
    "    \n",
    "    #Datetime\n",
    "    def deal_dtime(self):\n",
    "    \"\"\" TODO : Traiter les DateTime \"\"\"\n",
    "        pass    \n",
    "    #\n",
    "    # extract data (optionnal)\n",
    "    #\n",
    "    \n",
    "    def prepare_data(self, threshold: float):\n",
    "        self.separate_variable_types()\n",
    "        self.drop_uselessf()\n",
    "        self.deal_duplicate()\n",
    "        self.drop_nanp(threshold)\n",
    "        self.deal_dtime()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureRecipe:\n",
    "    \n",
    "    def __init__(self, data: pds.DataFrame):\n",
    "        print(\"FeatureRecipe starts...\")\n",
    "        self.df = data\n",
    "        self.cate = []\n",
    "        self.floa = []\n",
    "        self.intt = []\n",
    "        print(\"End of FeatureRecipe initialisation\\n\")\n",
    "    \n",
    "    def separate_variable_types(self) -> None:\n",
    "        print(\"Separate variable types starts...\")\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtypes == int:\n",
    "                self.intt.append(self.df[col])\n",
    "            elif self.df[col].dtypes == float:\n",
    "                self.floa.append(self.df[col])\n",
    "            else:\n",
    "                self.cate.append(self.df[col])\n",
    "        print(\"Separate variable types end...\")\n",
    "        print (\"Dataset number of columns : {} \\nnumber of discreet values : {} \\nnumber of continuous values : {} \\nnumber of others : {} \\ntotal size : {}\\n\".format(len(self.df.columns),\n",
    "        len(self.intt),len(self.floa),len(self.cate),len(self.intt)+len(self.floa)+len(self.cate) ))\n",
    "        \n",
    "    def drop_uselessf(self):\n",
    "        print(\"Drop useless feature start...\")\n",
    "        \n",
    "        if \"Unnamed: 0\" in self.df.columns:\n",
    "            self.df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "            \n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isna().sum() == len(self.df[col]):\n",
    "                self.df.drop([col], axis=1, inplace=True)\n",
    "                \n",
    "        print(\"Drop useless feature end...\")\n",
    "        print(\"Number columns remaining {}\\n\".format(len(self.df.columns)))\n",
    "        \n",
    "    def deal_duplicate(self):\n",
    "        print(\"Deal duplicate start...\")\n",
    "        dropped_duplicates = self.df.drop_duplicates(inplace=True)\n",
    "        print(\"Dropped duplicates : {}\".format(dropped_duplicates))\n",
    "        print(\"Deal duplicate end...\")\n",
    "    \n",
    "    def drop_nanp(self, threshold: float):\n",
    "        dropped = 0\n",
    "        print(\"Drop columns with {} percentage of NAN\".format(threshold))\n",
    "        self.get_duplicates()\n",
    "              \n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isna().sum() / self.df.shape[0] >= threshold:\n",
    "                self.df.drop([col], axis=1, inplace=True)\n",
    "                dropped+=1\n",
    "              \n",
    "        print(\"Number of columns dropped : {}\\n\".format(dropped))\n",
    "    \n",
    "    def get_duplicates(self):\n",
    "        print(\"Get duplicates\")\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            for row in range(self.df.shape[0]):\n",
    "                \n",
    "              \n",
    "    def deal_dtime(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_data(self, threshold: float):\n",
    "        self.drop_uselessf()\n",
    "        self.separate_variable_types()\n",
    "        self.deal_duplicate()\n",
    "        self.drop_nanp(threshold)\n",
    "        self.deal_dtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correction fonction Drop NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nanp(self, thresold: float):\n",
    "        \"\"\"\n",
    "        Drop NaN columns according to a thresold\n",
    "        \"\"\"\n",
    "        def deal_nanp(df:pd.DataFrame, thresold: float):\n",
    "            bf=[]\n",
    "            for c in self.data.columns.to_list():\n",
    "                if self.data[c].isna().sum()/self.data.shape[0] > thresold:\n",
    "                    bf.append(c)\n",
    "            logging.debug(\"{} feature have more than {} NaN \".format(len(bf), thresold))\n",
    "            logging.debug('\\n\\n - - - features - - -  \\n {}'.format(bf))\n",
    "            return bf \n",
    "        self.data = self.data.drop(deal_nanp(self.data, thresold), axis=1)\n",
    "        logging.debug('Some NaN features droped according to {} thresold'.format(thresold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fterecipe = FeatureRecipe(d1.df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fterecipe.prepare_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Classe FeatureExtractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature Extractor class\n",
    "    \"\"\"    \n",
    "    def __init__(self, data: pd.DataFrame, flist: list):\n",
    "        \"\"\"\n",
    "            Input : pandas.DataFrame, feature list to drop\n",
    "            Output : X_train, X_test, y_train, y_test according to sklearn.model_selection.train_test_split\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "        Feature Extractor class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data : pds.DataFrame, flist: list):\n",
    "        \"\"\"\n",
    "            Input : pandas.DataFrame, feature list to drop\n",
    "            Output : X_train, X_test, y_train, y_test according to sklearn.model_selection.train_test_split\n",
    "        \"\"\"\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.df = data\n",
    "        self.flist = flist\n",
    "        \n",
    "    def split(size: float, random_state :int = 42):\n",
    "        pass\n",
    "    \n",
    "    def train():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.equals(otherCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature Extractor class\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, flist: list):\n",
    "        \"\"\"\n",
    "            Input : pandas.DataFrame, feature list to drop\n",
    "            Output : X_train, X_test, y_train, y_test according to sklearn.model_selection.train_test_split\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None,None,None,None\n",
    "        self.df = data\n",
    "        self.flist = flist\n",
    "    def splitting(size:float,rng:int,X pd.Series):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test train_test_split(X, y, size, rng)\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature Extractor class\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, flist: list):\n",
    "        \"\"\"\n",
    "            Input : pandas.DataFrame, feature list to drop\n",
    "            Output : X_train, X_test, y_train, y_test according to sklearn.model_selection.train_test_split\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None,None,None,None\n",
    "        self.df = data\n",
    "        self.flist = flist\n",
    "    def splitting(size:float,rng:int,X pd.Series):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test train_test_split(X, y, size, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Classe ModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    \"\"\"\n",
    "        Class for train and print results of ml model \n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str = None, save: bool = None):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        pass\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        pass\n",
    "    def save_model(self, path:str):\n",
    "        #with the format : 'model_{}_{}'.format(date)\n",
    "        pass\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            #load model\n",
    "            pass\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataManager(d:DataHandler=None, fr: FeatureRecipe=None, fe:FeatureExtractor=None):\n",
    "    \"\"\"\n",
    "        Fonction qui lie les 4 classes de la pipeline et qui return FeatureExtractor.split(0.1)\n",
    "    \"\"\"\n",
    "    pass\n",
    "#on appelera la fonction DataManager() de la facon suivante : \n",
    "X_train, X_test, y_train, y_test = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ModelBuilder() \n",
    "m.train(X_train, y_train)\n",
    "m.print_accuracy()\n",
    "m.save_model(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
