{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78XCXkNIDPk2"
      },
      "source": [
        "# [Reconnaissance d'images](https://github.com/wikistat/Ateliers-Big-Data/tree/master/CatsVSDogs):  [*cats vs. dogs*](https://www.kaggle.com/c/dogs-vs-cats/data)\n",
        "# Tranfert d'apprentissage avec <a href=\"https://www.tensorflow.org/\"><img src=\"https://avatars0.githubusercontent.com/u/15658638?s=200&v=4\" width=100, style=\"display: inline\" alt=\"TensorFlow\"/></a> tensorflow et  <a href=\"https://keras.io/\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" width=250, style=\"display: inline\" alt=\"Keras\"/></a>\n",
        "\n",
        "### Résumé\n",
        "Apprentissage d'un réseau convolutionnel élémentaire puis utilisation de réseaux pré-entrainés (`VGG16`, `InceptionV3`) sur la base [ImageNet](http://www.image-net.org/) afin de résoudre un autre exemple de reconnaissance d'images. Utilisation de [Keras](https://keras.io/) pour piloter la librairie [tensorFlow](https://www.tensorflow.org/). Comparaison des performances des réseaux et des environnements de calcul CPU et GPU.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "### Objectifs\n",
        "La reconnaissance d'images a franchi une étape majeure en 2012. L'empilement de couches de neurones, dont certaines convolutionnelles, ont conduit à des algorithmes nettement plus performants en reconnaissance d'image, traitement du langage naturel, et à l'origine d'un battage médiatique considérable autour de l'apprentissage épais ou *deep learning*. Néanmoins, apprendre un réseau profond comportant des milions de paramètres nécessite une base d'apprentissage excessivement volumineuse (*e.g.* [ImageNet](http://www.image-net.org/)) avec des millions d'images labellisées. \n",
        "\n",
        "L'apprentissage s'avère donc très couteux en temps de calcul, même avec des technologies adaptées (GPU). Pour résoudre ce problème il est possible d'utiliser des réseaux *pré-entrainés*. Ces réseaux possèdent une structure particulière, établie de façon heuristique dans différents départements de recherche (Microsoft: Resnet, Google: Inception V3, Facebook: ResNet) avant d'être ajustés sur des banques d'images publiques telles que [ImageNet](http://www.image-net.org/). \n",
        "\n",
        "La stratégie de ce  *transfert d'apprentissage* consiste à exploiter la connaissance acquise sur un problème de classification général pour l’appliquer à un problème particulier.\n",
        "\n",
        "La librairie [Keras](https://keras.io/) permet de construire de tels réseaux en utlisant relativement simplement l'environnement  [tensorFlow](https://www.tensorflow.org/) de Google à partir de programmes récrits en Python. De plus Keras permet d'utiliser les performances d'une carte GPU afin d'atteindre des performances endant possible ce transfert d'apprentissage, même avec des réseaux complexes.\n",
        "\n",
        "L'objectif de ce tutoriel est de montrer les capacités du *transfert d'apprentissage* permettant de résoudre des problèmes complexes avec des moyens de calcul modestes. Néanmoins, une carte GPU est vivement conseillé.\n",
        "\n",
        "Ce tutoriel est en grande partie inspiré du [blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) de François Chollet à l'initiative de [Keras](https://keras.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNH2KOyBDPk6"
      },
      "source": [
        "## Environnement matériel et logiciel\n",
        "Keras et tensorFlow s'installent simplement à partir de la distribution Anaconda de Python. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EG2AKzpeDPk8"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Deep Learning Librairies\n",
        "import tensorflow as tf\n",
        "import keras.preprocessing.image as kpi\n",
        "import keras.layers as kl\n",
        "import keras.optimizers as ko\n",
        "import keras.backend as k\n",
        "import keras.models as km\n",
        "import keras.applications as ka\n",
        "\n",
        "# Visualisaiton des données\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbWncQ7JDPk-",
        "outputId": "1fafdad4-b124-4bf8-e6ed-4f708832a44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8999668158159346084\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14465892352\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 3756340665118000083\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhysFcmUDPk_",
        "outputId": "ff2f4efe-be6a-49ae-a20e-6071d1d2c3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ],
      "source": [
        "MODE = \"GPU\" if \"GPU\" in [k.device_type for k in device_lib.list_local_devices()] else \"CPU\"\n",
        "print(MODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riS3qTxxDPlB"
      },
      "source": [
        "## Prise en charge des données "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY-wfdxgDPlC"
      },
      "source": [
        "### Structure des données \n",
        "\n",
        "Les données originales peuvent être téléchargées à partir du site [kaggle](https://www.kaggle.com/c/dogs-vs-cats/data).\n",
        "L'ensemble d'apprentissage contient 25.000 images. C'est beaucoup trop pour des machines usuelles à moins de se montrer très patient.  Aussi, deux sous-échantillons d'apprentissage ont été créés et disposés dans le dépôt.\n",
        "\n",
        "1. 100 images de chats et 100 images de chiens plus un échantillon de validation consitué de 40 images de chats et 40 images de chien. \n",
        "2. 1000 images de chats et 1000 images de chiens plus un échantillon de validation consitué de 400 images de chats et 400 images de chien.\n",
        "\n",
        "Pour utiliser certaines fonctionnalités de Keras, les données doivent être organisées selon une abrorescence précise. Les fichiers appartenant à une même classe doivent être dans un même dossier. \n",
        "\n",
        "```\n",
        "data_dir\n",
        "└───subsample/\n",
        "│   └───train/\n",
        "│   │   └───cats/\n",
        "│   │   │   │   cat.0.jpg\n",
        "│   │   │   │   cat.1.jpg\n",
        "│   │   │   │   ...\n",
        "│   │   └───dogs/\n",
        "│   │   │   │   dog.0.jpg\n",
        "│   │   │   │   dog.1.jpg\n",
        "│   │   │   │   ...\n",
        "│   └───test/\n",
        "│   │   └───cats/\n",
        "│   │   │   │   cat.1000.jpg\n",
        "│   │   │   │   cat.1000.jpg\n",
        "│   │   │   │   ...\n",
        "│   │   └───dogs/\n",
        "│   │   │   │   dog.1000.jpg\n",
        "│   │   │   │   dog.1000.jpg\n",
        "│   │   │   │   ...\n",
        "```\n",
        "\n",
        "*N.B.* Des sous-échantillons plus importants créés à partir des données originales doivent être enregistrés en respectant scrupuleusement cette structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzd-kui7DPlD"
      },
      "source": [
        "###  Création d'un jeu d'apprentissage et de validation\n",
        "Spécifier le chemin du dossier contenant les données, si ce n'est pas le répertoire courant, ainsi que les tailles des échantillons d'apprentissage et de validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bfPxgbugDPlE"
      },
      "outputs": [],
      "source": [
        "data_dir = '' # chemin d'accès aux données\n",
        "\n",
        "N_train = 200 #2000\n",
        "N_val = 80 #800\n",
        "\n",
        "data_dir_sub = data_dir+'subsample_%d_Ntrain_%d_Nval' %(N_train, N_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcxYNaB5DPlF"
      },
      "source": [
        "### Illustration des données\n",
        "\n",
        "La fonction `load_img` permet de charger une image comme une image PIL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "8ZIvEWb5DPlF",
        "outputId": "c822d064-e7c9-4a80-eacc-05ec003fee4e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-90568151046e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir_sub\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/train/cats/cat.1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# this is a PIL image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m    313\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 314\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'subsample_200_Ntrain_80_Nval/train/cats/cat.1.jpg'"
          ]
        }
      ],
      "source": [
        "img = kpi.load_img(data_dir_sub+'/train/cats/cat.1.jpg')  # this is a PIL image\n",
        "img "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-IpHTnlDPlG"
      },
      "source": [
        "La fonction `img_to_array` génére un `array numpy` a partir d'une image PIL ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "YLWVIo2DDPlH",
        "outputId": "44d4840a-2c37-4f2b-9547-ffa318b8bc38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8f7ca4fa5a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
          ]
        }
      ],
      "source": [
        "x = kpi.img_to_array(img)  \n",
        "plt.imshow(x/255, interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRzk6LfSDPlH"
      },
      "source": [
        "### Pré-traitements\n",
        "\n",
        "Les images du jeu de données sont de dimensions différentes : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "BGedSWzdDPlH",
        "outputId": "542f196b-fa1a-4238-e08c-3f38702449bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-96569de6d560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir_sub\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/train/cats/cat.0.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir_sub\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/train/cats/cat.1.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m    313\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 314\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'subsample_200_Ntrain_80_Nval/train/cats/cat.0.jpg'"
          ]
        }
      ],
      "source": [
        "x_0 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.0.jpg\"))\n",
        "x_1 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\"))\n",
        "x_0.shape, x_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QILIQXMqDPlI"
      },
      "outputs": [],
      "source": [
        "datagen = kpi.ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "72YafMHUDPlJ"
      },
      "outputs": [],
      "source": [
        "img_width = 150\n",
        "img_height = 150\n",
        "\n",
        "img = kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\")  # this is a PIL image\n",
        "x = kpi.img_to_array(img)  \n",
        "x_ = x.reshape((1,) + x.shape)\n",
        "\n",
        "if not(os.path.isdir(data_dir_sub+\"/preprocessing_example\")):\n",
        "    os.mkdir(data_dir_sub+\"/preprocessing_example\")\n",
        "\n",
        "    i = 0\n",
        "    for batch in datagen.flow(x_, batch_size=1,save_to_dir=data_dir_sub+\"/preprocessing_example\", save_prefix='cat', save_format='jpeg'):\n",
        "        i += 1\n",
        "        if i > 7:\n",
        "            break  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "orhKnIqeDPlJ"
      },
      "outputs": [],
      "source": [
        "X_list=[]\n",
        "for f in os.listdir(data_dir_sub+\"/preprocessing_example\"):\n",
        "    X_list.append(kpi.img_to_array(kpi.load_img(data_dir_sub+\"/preprocessing_example/\"+f)))\n",
        "\n",
        "\n",
        "fig=plt.figure(figsize=(16,8))\n",
        "fig.patch.set_alpha(0)\n",
        "ax = fig.add_subplot(3,3,1)\n",
        "ax.imshow(x/255, interpolation=\"nearest\")\n",
        "ax.set_title(\"Image original\")\n",
        "for i,xt in enumerate(X_list):\n",
        "    ax = fig.add_subplot(3,3,i+2)\n",
        "    ax.imshow(xt/255, interpolation=\"nearest\")\n",
        "    ax.set_title(\"Random transformation %d\" %(i+1))\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cats_transformation.png\", dpi=100, bbox_to_anchor=\"tight\", facecolor=fig.get_facecolor())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt3RJnIODPlK"
      },
      "source": [
        "## Classification d'image à l'aide du Deep Learning\n",
        "\n",
        "Dans un premier temps, nous allons fixer le nombre d'epochs ainsi que la taille de notre batch afin que ces deux paramètres soit communs aux différentes méthodes que nous allons tester. \n",
        "Queques règles à suivre pour le choix de ces paramètres :\n",
        "\n",
        "* `epochs`: Commencer avec un nombre d'epochs relativement faible (2,3) afin de voir le temps de calcul nécessaire à votre machine, puis augmenter le en conséquence.\n",
        "* `batch_size`: La taille du batch correspond au nombre d'éléments qui seront étudié a chaque itération au cours d'une epochs. \n",
        "\n",
        "\n",
        "**Important** : Avec Keras, lorsque les données sont générés avec un générateur (voir précédemment) la taille du batch doit être un diviseur de la taille de l'échantillon. Sinon l'algorithme aura des comportement anormaux qui ne généreront pas forcément un message d'erreur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-mUgfVSpDPlK"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "batch_size=20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulwk4lckDPlK"
      },
      "source": [
        "### Réseau convolutionnel\n",
        "\n",
        "Dans un premiers temps, on construit notre propre réseau de neurones convolutionnel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lv012AvDPlK"
      },
      "source": [
        "#### Génération des données\n",
        "\n",
        "On définit deux objets `ImageDataGenerator` :\n",
        "\n",
        "* `train_datagen`: pour l'apprentissage, où différentes transformations sont appliquées, comme précédement\n",
        "* `valid_datagen`: pour la validation, où l'on applique seulement une transformation *rescale* pour ne pas déformer les données.\n",
        "\n",
        "Il est également important de définir la taille des images dans laquelle nos images seront reformatées. Ici nous choisirons un taille d'image de 150x150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MhoniPIfDPlL"
      },
      "outputs": [],
      "source": [
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = kpi.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        ")\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "valid_datagen = kpi.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        data_dir_sub+\"/train/\",  # this is the target directory\n",
        "        target_size=(img_width, img_height),  \n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = valid_datagen.flow_from_directory(\n",
        "        data_dir_sub+\"/validation/\",\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLdSu1aVDPlL"
      },
      "source": [
        "#### Définition du modèle\n",
        "\n",
        "Le modèle est consitué de 3 blocs de convolution consitutés chacun de:\n",
        "\n",
        "* Une couche de `Convolution2D`\n",
        "* Une couche d'`Activation` ReLU\n",
        "* Une couche `MaxPooling2D`\n",
        "\n",
        "Suivi de :\n",
        "* Une couche `Flatten`, permettant de convertir les features de 2 à 1 dimensions. \n",
        "* Une couche `Dense` (Fully connected layer)\n",
        "* Une couche d' `Activation` ReLU\n",
        "* Une couche `Dropout`\n",
        "* Une couche `Dense` de taille 1 suivi d'une `Activation` sigmoid permettant la classification binaire\n",
        "\n",
        "On utilise la fonction de perte `binary_crossentropy` pour apprendre notre modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "s5-rPoqgDPlL"
      },
      "outputs": [],
      "source": [
        "model_conv = km.Sequential()\n",
        "model_conv.add(kl.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), data_format=\"channels_last\"))\n",
        "model_conv.add(kl.Activation('relu'))\n",
        "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_conv.add(kl.Conv2D(32, (3, 3)))\n",
        "model_conv.add(kl.Activation('relu'))\n",
        "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_conv.add(kl.Conv2D(64, (3, 3)))\n",
        "model_conv.add(kl.Activation('relu'))\n",
        "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_conv.add(kl.Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model_conv.add(kl.Dense(64))\n",
        "model_conv.add(kl.Activation('relu'))\n",
        "model_conv.add(kl.Dropout(0.5))\n",
        "model_conv.add(kl.Dense(1))\n",
        "model_conv.add(kl.Activation('sigmoid'))\n",
        "\n",
        "model_conv.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model_conv.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI7JxyCDDPlM"
      },
      "source": [
        "#### Apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E-4T263mDPlN"
      },
      "outputs": [],
      "source": [
        "ts = time.time()\n",
        "model_conv.fit_generator(train_generator, steps_per_epoch=N_train // batch_size, epochs=epochs,\n",
        "       validation_data=validation_generator,validation_steps=N_val // batch_size)\n",
        "te = time.time()\n",
        "t_learning_conv_simple_model = te-ts\n",
        "print(\"Learning TIme for %d epochs : %d seconds\"%(epochs,t_learning_conv_simple_model))\n",
        "model_conv.save(data_dir_sub+'/'+MODE+'_models_convolutional_network_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-D2U8V0DPlO"
      },
      "source": [
        "#### Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AzUBpLsUDPlO"
      },
      "outputs": [],
      "source": [
        "ts = time.time()\n",
        "score_conv_val = model_conv.evaluate_generator(validation_generator, N_val /batch_size, verbose=1)\n",
        "score_conv_train = model_conv.evaluate_generator(train_generator, N_train / batch_size, verbose=1)\n",
        "te = time.time()\n",
        "t_prediction_conv_simple_model = te-ts\n",
        "print('Train accuracy:', score_conv_train[1])\n",
        "print('Validation accuracy:', score_conv_val[1])\n",
        "print(\"Time Prediction: %.2f seconds\" %t_prediction_conv_simple_model )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq5YKGLmDPlO"
      },
      "source": [
        "**Q** Commentez les valeurs de prédictions d'apprentissage et de validation. Comparez les avec les résultats de la dernière epochs d'apprentissage. Qu'observez vous? Est-ce normal?\n",
        "\n",
        "**Exercice** Re-faites tournez ce modèle en ajoutant plus de transformation aléatoire dans le générateur d'image au moment de l'apprentissage. Que constatez-vous?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFUzQi48DPlP"
      },
      "source": [
        "### Réseau pré-entrainé : VGG16\n",
        "\n",
        "Nous allons voir dans cette partie deux façon d'utiliser un modèle pré-entrainé:\n",
        "\n",
        "1. Dans un premier temps on utilise le modèle pour extraire des features des images qui seront utilisés dans un réseaux de convolution \"classique\". Ces features sont le résultats des transformations des différents blocs de convolution sur nos images.  \n",
        "2. Dans un second temps on branchera le modèle \"classique\" généré  directement sur le modèle pré-entrainé. Ce modèle sera ensuite ré-entraîné plus finement (Fine Tuning) sur le dernier bloc de convolution.\n",
        "\n",
        "\n",
        "#### Illustration du réseau\n",
        "\n",
        "![](https://blog.keras.io/img/imgclf/vgg16_original.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0u4b6cwDPlP"
      },
      "source": [
        "#### Extraction de nouvelle caractéristiques (*features*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN8XnvLrDPlP"
      },
      "source": [
        "##### Téléchargement des poids du modèle\n",
        "\n",
        "Si cest la première fois que vous appeler l'application `VGG16`, le lancement des poids commencera automatiquement et seront stocké dans votre home : `\"~/.keras/models\"`\n",
        "\n",
        "On utilise le modèle avec l'option `ìnclude_top` = False. C'est à dire que l'on ne télécharge pas le dernier bloc `Fully connected` classifier. \n",
        "\n",
        "La fonction `summary` permet de retrouver la structure décrite précédemment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hxBhoVG8DPlQ"
      },
      "outputs": [],
      "source": [
        "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet')\n",
        "model_VGG16_without_top.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAuEYQPDPlQ"
      },
      "source": [
        "##### Création des caractéristiques\n",
        "\n",
        "On applique alors les 5 blocs du modèle VGG16 sur les images de nos échantillons d'apprentissage et de validation.\n",
        "\n",
        "Cette opération peut-être couteuse, c'est pourquoi on va sauver ces features dans des fichiers afin d'effectuer qu'une fois cette opération.\n",
        "Si ces fichiers existent, les poids seront téléchargés, sinon il seront créés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ku4MR1U0DPlQ"
      },
      "outputs": [],
      "source": [
        "features_train_path = data_dir_sub+'/features_train.npy' \n",
        "features_validation_path = data_dir_sub+'/features_validation.npy' \n",
        "\n",
        "if os.path.isfile(features_train_path) and os.path.isfile(features_validation_path):\n",
        "    print(\"Load Features\")\n",
        "    features_train = np.load(open(features_train_path, \"rb\"))\n",
        "    features_validation = np.load(open(features_validation_path, \"rb\"))\n",
        "    \n",
        "else:\n",
        "    print(\"Generate Features\")\n",
        "    datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "            data_dir_sub+\"/train\",\n",
        "            target_size=(img_width, img_height),\n",
        "            batch_size=batch_size,\n",
        "            class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
        "            shuffle=False)  \n",
        "    features_train = model_VGG16_without_top.predict_generator(generator, N_train / batch_size,  verbose = 1)\n",
        "    # save the output as a Numpy array\n",
        "    np.save(open(features_train_path, 'wb'), features_train)\n",
        "\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        data_dir_sub+\"/validation\",\n",
        "            target_size=(img_width, img_height),\n",
        "            batch_size=batch_size,\n",
        "            class_mode=None,\n",
        "            shuffle=False)\n",
        "    features_validation = model_VGG16_without_top.predict_generator(generator, N_val / batch_size,  verbose = 1)\n",
        "    # save the output as a Numpy array\n",
        "    np.save(open(features_validation_path, 'wb'), features_validation)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psajVsTqDPlQ"
      },
      "source": [
        "##### Construction d'un réseaux de neurone classique.\n",
        "\n",
        "On construit un réseaux de neurones \"classique\", identique à la seconde partie du réseau précédent. \n",
        "\n",
        "*Attention* : La première couche de ce réseaux (`Flatten`) doit être configurée pour prendre en compte des données dans la dimension des features générées précédemment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZkaEaxX3DPlR"
      },
      "outputs": [],
      "source": [
        "model_VGG_fcm = km.Sequential()\n",
        "model_VGG_fcm.add(kl.Flatten(input_shape=features_train.shape[1:]))\n",
        "model_VGG_fcm.add(kl.Dense(64, activation='relu'))\n",
        "model_VGG_fcm.add(kl.Dropout(0.5))\n",
        "model_VGG_fcm.add(kl.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_VGG_fcm.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_VGG_fcm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uya-ZQvmDPlR"
      },
      "source": [
        "##### Apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OUH19uboDPlR"
      },
      "outputs": [],
      "source": [
        "# On créer des vecteurs labels\n",
        "\n",
        "train_labels = np.array([0] * int((N_train/2)) + [1] * int((N_train/2)))\n",
        "validation_labels = np.array([0] * int((N_val/2)) + [1] * int((N_val/2)))\n",
        "\n",
        "model_VGG_fcm.fit(features_train, train_labels,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=(features_validation, validation_labels))\n",
        "t_learning_VGG_fcm = te-ts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkoPkjyKDPlR"
      },
      "source": [
        "**Q** Commentez les performances de ce nouveau modèle\n",
        "\n",
        "Nous allons également sauver les poids de ce modèle afin de les réusiliser dans la prochaine partie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0g5bDsi9DPlR"
      },
      "outputs": [],
      "source": [
        "model_VGG_fcm.save_weights(data_dir_sub+'/weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHysaKceDPlS"
      },
      "source": [
        "##### Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6gDm6NmUDPlS"
      },
      "outputs": [],
      "source": [
        "ts = time.time()\n",
        "score_VGG_fcm_val = model_VGG_fcm.evaluate(features_validation, validation_labels)\n",
        "score_VGG_fcm_train = model_VGG_fcm.evaluate(features_train, train_labels)\n",
        "te = time.time()\n",
        "t_prediction_VGG_fcm = te-ts\n",
        "print('Train accuracy:', score_VGG_fcm_train[1])\n",
        "print('Validation accuracy:', score_VGG_fcm_val[1])\n",
        "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_fcm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzahyTklDPlS"
      },
      "source": [
        "### Ajustement fin du réseau VGG16\n",
        "\n",
        "Dans la partie précédente, nous avons configurer un bloc de réseaux de neurones, à même de prendre en entrée les features issues des transformation des 5 premiers blocs de convolution du modèle VGG16. \n",
        "\n",
        "Dans cette partie, nous allons 'brancher' ce bloc directement sur les cinq premiers blocs du modèle VGG16 pour pouvoir affiner le modèle en itérant a la fois sur les blocs de convolution mais également sur notre bloc de réseau de neurone.\n",
        "\n",
        "![](https://blog.keras.io/img/imgclf/vgg16_modified.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBtIV2dDPlS"
      },
      "source": [
        "#### Création du modèle\n",
        "\n",
        "On télécharge dans un premier temps le modèle VGG16, comme précédement. \n",
        "Cependant, le modèle va cette fois être \"entrainé\" directement. Il ne va pas servir qu'a générer des features. Il faut donc préciser en paramètre la taille des images que l'on va lui donner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YsqIK0JEDPlS"
      },
      "outputs": [],
      "source": [
        "# build the VGG16 network\n",
        "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
        "print('Model loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdladm4BDPlS"
      },
      "source": [
        "On ajoute au modèle VGG, notre bloc de réseaux de neuronne construit précédemment pour générer des features. \n",
        "Pour cela, on construit le bloc comme précédemment, puis on y ajoute les poids issus de l'apprentissage réalisé précédemment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "xa8vYRUIDPlT"
      },
      "outputs": [],
      "source": [
        "# build a classifier model to put on top of the convolutional model\n",
        "top_model = km.Sequential()\n",
        "top_model.add(kl.Flatten(input_shape=model_VGG16_without_top.output_shape[1:]))\n",
        "top_model.add(kl.Dense(64, activation='relu'))\n",
        "top_model.add(kl.Dropout(0.5))\n",
        "top_model.add(kl.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# note that it is necessary to start with a fully-trained\n",
        "# classifier, including the top classifier,\n",
        "# in order to successfully do fine-tuning\n",
        "\n",
        "top_model.load_weights(data_dir_sub+'/weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti8_d8vHDPlT"
      },
      "source": [
        "Enfin on assemble les deux parties du modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lWVkOf4MDPlT"
      },
      "outputs": [],
      "source": [
        "# add the model on top of the convolutional base\n",
        "model_VGG_LastConv_fcm = km.Model(inputs=model_VGG16_without_top.input, outputs=top_model(model_VGG16_without_top.output))\n",
        "\n",
        "model_VGG_LastConv_fcm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHo-Y9sDPlT"
      },
      "source": [
        "#### Gèle des 4 premiers blocs de convolution\n",
        "\n",
        "En pratique, et pour pouvoir effectuer ces calculs dans un temps raisonable, nous allons \"fine-tuner\" seulement le dernier bloc de convolution du modèle, le bloc 5 (couches 16 à 19 dans le summary du modèle précédent) ainsi que le bloc de réseau de neurones que nous avons ajoutés. \n",
        "\n",
        "Pour cela on va \"geler\" (Freeze) les 15 premières couches du modèle pour que leur paramètre ne soit pas optimiser pendant la phase d'apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "izpxI4moDPlU"
      },
      "outputs": [],
      "source": [
        "for layer in model_VGG_LastConv_fcm.layers[:15]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoW9GYf-DPlU"
      },
      "source": [
        "#### Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3evWLEpLDPlU"
      },
      "outputs": [],
      "source": [
        "# prepare data augmentation configuration\n",
        "train_datagen = kpi.ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir_sub+\"/train/\",\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    data_dir_sub+\"/validation/\",\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc0DestLDPlU"
      },
      "source": [
        "#### Apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fc_R7L4RDPlU"
      },
      "outputs": [],
      "source": [
        "model_VGG_LastConv_fcm.compile(loss='binary_crossentropy',\n",
        "              optimizer=ko.SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# fine-tune the model\n",
        "ts = time.time()\n",
        "model_VGG_LastConv_fcm.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=N_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=N_val // batch_size)\n",
        "te = time.time()\n",
        "t_learning_VGG_LastConv_fcm = te-ts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he_yWgKxDPlV"
      },
      "source": [
        "####  Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tsNQO9W4DPlV"
      },
      "outputs": [],
      "source": [
        "ts = time.time()\n",
        "score_VGG_LastConv_fcm_val = model_VGG_LastConv_fcm.evaluate_generator(validation_generator, N_val // batch_size)\n",
        "score_VGG_LastConv_fcm_train = model_VGG_LastConv_fcm.evaluate_generator(train_generator, N_train // batch_size)\n",
        "\n",
        "te = time.time()\n",
        "t_prediction_VGG_LastConv_fcm = te-ts\n",
        "print('Train accuracy:', score_VGG_LastConv_fcm_val[1])\n",
        "print('Validation accuracy:', score_VGG_LastConv_fcm_train[1])\n",
        "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_LastConv_fcm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks_Tm6z_DPlV"
      },
      "source": [
        "###  Autres modèles\n",
        "\n",
        "Keras possède un certain nombre d'autres modèles pré-entrainés:\n",
        "\n",
        "* Xception\n",
        "* VGG16\n",
        "* VGG19\n",
        "* ResNet50\n",
        "* InceptionV3\n",
        "* InceptionResNetV2\n",
        "* MobileNet\n",
        "\n",
        "Certains possèdent une structure bien plus complexe, notamment `InceptionV3`. Vous pouvez très facilement remplacer la fonction `ka.VGG16` par une autre fonction (ex : `ka.InceptionV3`) pour tester la performance des ces différents modèles et leur complexité."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDilQpPGDPlV"
      },
      "source": [
        "**Exercice** Vous pouvez re-effectuer les manipulations précédentes sur d'autres modèle pré-entrainé, en prenant le temps d'étudiez leur architecture.\n",
        "\n",
        "**Exercice** Vous pouvez également re-effectuer ces apprentissage sur un jeu de données plus gros en en créant un nouveau à partir des données originales. \n",
        "\n",
        "L'application de ces exercices sur les données du challenge est vivement conseillées :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ0jxoxlDPlV"
      },
      "source": [
        "### Prédiction sur le jeu test de Kaggle\n",
        "\n",
        "Voyons à présent comment notre réseau performe sur un échantillon du dataset test de keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA8dykZdDPlV"
      },
      "outputs": [],
      "source": [
        "data_dir_test = data_dir+'test/'\n",
        "N_test = len(os.listdir(data_dir_test+\"/test\"))\n",
        "\n",
        "test_datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    data_dir_test,\n",
        "    #data_dir_sub+\"/train/\",\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=False)\n",
        "\n",
        "test_prediction = model_VGG_LastConv_fcm.predict_generator(test_generator, N_test // batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3jMhLQqDPlW"
      },
      "outputs": [],
      "source": [
        "images_test = [data_dir_test+\"/test/\"+k for k in os.listdir(data_dir_test+\"/test\")][:9]\n",
        "x_test  = [kpi.img_to_array(kpi.load_img(image_test))/255 for image_test in images_test]  # this is a PIL image\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "for k in range(9):\n",
        "    ax = fig.add_subplot(3,3,k+1)\n",
        "    ax.imshow(x_test[k], interpolation='nearest')\n",
        "    pred = test_prediction[k]\n",
        "    if pred >0.5:\n",
        "        title = \"Probabiliy for dog : %.1f\" %(pred*100)\n",
        "    else:\n",
        "        title = \"Probabiliy for cat : %.1f\" %((1-pred)*100)\n",
        "    ax.set_title(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bxvQX9FDPlW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "865px",
        "left": "0px",
        "right": "1587.01px",
        "top": "106px",
        "width": "213px"
      },
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Atelier-keras-CatsVSDogs.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}